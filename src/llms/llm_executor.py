# _*_ encoding: utf-8 _*_
# executor: use Gemini to execute plans and generate solutions
import os
from dotenv import load_dotenv, find_dotenv
from typing import List, Dict
from langchain_google_genai import ChatGoogleGenerativeAI
from pathlib import Path
from tools.read_md import read_markdown_file

load_dotenv(find_dotenv())
GEMINI_MODEL_ID = os.getenv("GEMINI_MODEL_ID")
MODEL_API_KEY = os.getenv("MODEL_API_KEY")
MODEL_API_URL = os.getenv("GEMINI_API_URL")
EXECUTOR_PROMPT_PATH = Path(__file__).parent.parent.joinpath("prompts", "Executor_Prompts.md")

class ExecutorModel:
    def __init__(self, model_name=GEMINI_MODEL_ID, api_key=MODEL_API_KEY, base_url=MODEL_API_URL):
        self.mode_name = model_name
        self.api_key = api_key
        self.base_url = base_url
        if not all ([self.mode_name, self.api_key, self.base_url]):
            raise ValueError("æ¨¡å‹IDã€APIå¯†é’¥å’ŒæœåŠ¡åœ°å€å¿…é¡»è¢«æä¾›æˆ–åœ¨.envæ–‡ä»¶ä¸­å®šä¹‰ã€‚")
        self.model = ChatGoogleGenerativeAI(
            model=self.mode_name,
            api_key=self.api_key,
            base_url=self.base_url,
            temperature=1.0,  # Gemini 3.0+ defaults to 1.0
            max_tokens=None,
            timeout=60,
            max_retries=2
        )

    def __think(self, messages: List[Dict[str, str]]) -> str:
        """
        è°ƒç”¨å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œæ€è€ƒï¼Œå¹¶è¿”å›å…¶å“åº”ã€‚
        """
        max_retries = 3
        for attempt in range(max_retries):
            print(f"ğŸ§  æ­£åœ¨è°ƒç”¨ {self.model} æ¨¡å‹ (å°è¯• {attempt + 1}/{max_retries})...")
            try:
                model = self.model
                response = model.invoke(messages)
                # å¤„ç†éæµå¼å“åº”
                print("âœ… å¤§è¯­è¨€æ¨¡å‹å“åº”æˆåŠŸ:") if response else print("âš ï¸ è­¦å‘Š: å¤§è¯­è¨€æ¨¡å‹æœªè¿”å›å“åº”ã€‚")
                content = response.text if response else "[Warning]: No response received from the model."
                print(content)
                return content

            except Exception as e:
                print(f"âŒ è°ƒç”¨LLM APIæ—¶å‘ç”Ÿé”™è¯¯: {e}")
                if attempt < max_retries - 1:
                    print("âš ï¸ æ­£åœ¨é‡è¯•...")
        return None

    def execute(self, question: str, plan: list[str]) -> str:
        """
        æ ¹æ®è®¡åˆ’ï¼Œé€æ­¥æ‰§è¡Œå¹¶è§£å†³é—®é¢˜ã€‚
        """
        try:
            history = "" # ç”¨äºå­˜å‚¨å†å²æ­¥éª¤å’Œç»“æœçš„å­—ç¬¦ä¸²
            EXECUTOR_PROMPT_TEMPLATE = read_markdown_file(EXECUTOR_PROMPT_PATH)
            print("\n--- æ­£åœ¨æ‰§è¡Œè®¡åˆ’ ---")
            
            for i, step in enumerate(plan):
                print(f"\n-> æ­£åœ¨æ‰§è¡Œæ­¥éª¤ {i+1}/{len(plan)}: {step}")
                
                prompt = EXECUTOR_PROMPT_TEMPLATE.format(
                    question=question,
                    plan=plan,
                    history=history if history else "æ— ", # å¦‚æœæ˜¯ç¬¬ä¸€æ­¥ï¼Œåˆ™å†å²ä¸ºç©º
                    current_step=step
                )
                
                messages = [{"role": "user", "content": prompt}]
                
                response_text = self.__think(messages=messages) or ""
                
                # æ›´æ–°å†å²è®°å½•ï¼Œä¸ºä¸‹ä¸€æ­¥åšå‡†å¤‡
                history += f"æ­¥éª¤ {i+1}: {step}\nç»“æœ: {response_text}\n\n"
                
                print(f"âœ… æ­¥éª¤ {i+1} å·²å®Œæˆï¼Œç»“æœ: {response_text}")

            # å¾ªç¯ç»“æŸåï¼Œæœ€åä¸€æ­¥çš„å“åº”å°±æ˜¯æœ€ç»ˆç­”æ¡ˆ
            final_answer = response_text
            return final_answer
        except FileNotFoundError as e:
            return f"Error: Executor prompt file not found. {e}"
        except Exception as e:
            return f"âŒ æ‰§è¡Œè®¡åˆ’æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"